# ğŸ§ª Experiment 7: Transformer-Based Encoder-Decoder for English-to-Spanish Translation

## ğŸ“Œ Objective

To implement a complete Transformer-based Encoder-Decoder model from scratch for English-to-Spanish translation and compare its performance with LSTM-based Seq2Seq models.

---

## ğŸ“‚ Dataset

- Dataset: `spa.txt`
- Contains Englishâ€“Spanish sentence pairs.
- Preprocessed using:
  - Lowercasing
  - Special character cleaning
  - Tokenization
- Split into:
  - 80% Training
  - 10% Validation
  - 10% Testing

---

## ğŸ—ï¸ Model Architecture

The Transformer model was implemented manually (without using `nn.Transformer`) and includes:

### 1ï¸âƒ£ Embedding Layer
- Learned embeddings for both source (English) and target (Spanish).

### 2ï¸âƒ£ Positional Encoding
- Sinusoidal positional encoding.
- Added to embeddings to retain word order information.

### 3ï¸âƒ£ Scaled Dot-Product Attention
\[
Attention(Q,K,V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]

### 4ï¸âƒ£ Multi-Head Attention
- Multiple attention heads
- Linear projections for Q, K, V
- Concatenation and final projection

### 5ï¸âƒ£ Transformer Encoder
Each encoder layer includes:
- Multi-head self-attention
- Feed Forward Network
- Residual connections
- Layer Normalization
- Stacked encoder layers

### 6ï¸âƒ£ Transformer Decoder
Each decoder layer includes:
- Masked multi-head self-attention
- Encoderâ€“Decoder (cross) attention
- Feed Forward Network
- Residual connections
- Layer Normalization
- Stacked decoder layers

### 7ï¸âƒ£ Masking
- Padding mask for padded tokens
- Causal mask for decoder (prevents looking ahead)

---

## âš™ï¸ Training Details

- Optimizer: Adam
- Loss Function: CrossEntropyLoss (ignore padding index)
- Teacher Forcing: Implemented via shifted target input
- Batch Size: 32
- Epochs: 3â€“5 (adjustable)
- Reduced dataset size for Kaggle performance

---

## ğŸ“Š Evaluation Metrics

### 1ï¸âƒ£ BLEU Score
Used to evaluate translation quality.

### 2ï¸âƒ£ Training Loss
Loss curve plotted to observe convergence.

### 3ï¸âƒ£ Training Time
Measured total training duration.

---

## ğŸ“ˆ Performance Comparison

| Model                  | BLEU Score | 
|------------------------|------------|
| Vanilla LSTM           | 0.0189     |
| Bahdanau Attention     | 0.0230     | 
| Luong Attention        | 0.0216     |
| Transformer            | 0.0347     |

> Transformer generally achieves higher BLEU due to parallel self-attention and better long-range dependency modeling.

---

## ğŸ” Example Translation Outputs

Example:
`Source: stop moving.`
`Reference: deja de moverte.`
`Predicted: deja de moverte.`


The Transformer demonstrates contextual understanding and improved translation fluency compared to LSTM-based models.

---

## ğŸš€ Key Advantages of Transformer

- Parallel computation (faster training)
- Better long-range dependency modeling
- Multi-head attention captures diverse relationships
- State-of-the-art architecture for translation tasks

---

## ğŸ“ Project Structure

```
Experiment7/
â”‚
â”œâ”€â”€ eng_spa.txt
â”œâ”€â”€ experiment7.ipynb
â”œâ”€â”€ README.md
```

---

## ğŸ Conclusion

The manually implemented Transformer-based Encoder-Decoder successfully performs English-to-Spanish translation. 

Compared to LSTM-based Seq2Seq models:
- Transformer achieved higher BLEU score
- Training was more parallelized
- Better contextual alignment was observed

This experiment demonstrates the effectiveness of attention-based architectures in sequence-to-sequence learning.

---

## ğŸ‘©â€ğŸ’» Author

Nisha Singh  
M.Tech AI / Deep Learning Lab
