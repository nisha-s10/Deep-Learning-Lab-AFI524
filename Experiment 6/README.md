# ğŸ§ª Experiment 6  
# Encoderâ€“Decoder Architecture with Bahdanau & Luong Attention

---

## ğŸ“Œ Objective

The objective of this experiment is to implement a complete Sequence-to-Sequence (Seq2Seq) architecture for machine translation using:

- Vanilla LSTM Encoderâ€“Decoder  
- Bahdanau (Additive) Attention  
- Luong (Multiplicative) Attention  

The performance of all three models is compared using BLEU score evaluation and attention visualization.

---

## ğŸ“‚ Dataset

The dataset consists of Englishâ€“Spanish sentence pairs in tab-separated format:

Hello. Hola.
How are you? Â¿CÃ³mo estÃ¡s?


### Data Split
- 80% Training  
- 10% Validation  
- 10% Testing  

---

## âš™ï¸ Implementation Steps

### 1ï¸âƒ£ Data Preprocessing
- Lowercasing
- Removing special characters
- Tokenization
- Vocabulary creation
- Special tokens:
  - `<pad>`
  - `<sos>`
  - `<eos>`
  - `<unk>`

---

### 2ï¸âƒ£ Dataset Preparation
- Sequence encoding
- Padding to fixed `MAX_LEN`
- DataLoader with batching

---

### 3ï¸âƒ£ Model Architectures

#### ğŸ”¹ Vanilla Encoderâ€“Decoder
- LSTM Encoder
- LSTM Decoder
- Teacher Forcing

#### ğŸ”¹ Bahdanau Attention
Additive attention mechanism:

score = váµ€ tanh(Wâ‚háµ¢ + Wâ‚‚sâ‚œ)

#### ğŸ”¹ Luong Attention
Multiplicative attention mechanism:

score = hâ‚œáµ€ W hâ‚›

---

### 4ï¸âƒ£ Training
- Optimizer: Adam  
- Loss Function: CrossEntropyLoss (ignoring `<pad>`)  
- Teacher Forcing Ratio: 0.5  
- Multiple epochs  

---

### 5ï¸âƒ£ Evaluation

Evaluation Metric:

- **BLEU Score**

Each model is evaluated on the test dataset.

---

### 6ï¸âƒ£ Attention Visualization

Attention weights are visualized using heatmaps to show:

- Alignment between source and target tokens  
- Decoder focus during translation  

---

## ğŸ“Š Results Comparison

| Model | BLEU Score | Performance |
|-------|------------|-------------|
| Vanilla LSTM | Lower | Struggles with long dependencies |
| Bahdanau Attention | Higher | Better alignment |
| Luong Attention | Comparable / Slightly better | Efficient attention scoring |

---

## ğŸ§  Key Observations

- Attention improves translation quality significantly.
- Vanilla Seq2Seq struggles due to fixed context vector.
- Bahdanau attention improves alignment quality.
- Luong attention is computationally efficient.

---

## ğŸš€ Technologies Used

- Python
- PyTorch
- NumPy
- NLTK
- Matplotlib
- Seaborn
- Scikit-learn

---
## ğŸ“ Project Structure

```
Experiment6/
â”‚
â”œâ”€â”€ spa.txt
â”œâ”€â”€ experiment6.ipynb
â”œâ”€â”€ README.md
```


---

## â–¶ï¸ How to Run

1. Install dependencies:

pip install torch nltk seaborn matplotlib scikit-learn


2. Place `eng_spa.txt` inside the project folder.

3. Run the notebook or Python script.

---

## ğŸ¯ Conclusion

This experiment demonstrates how attention mechanisms enhance sequence-to-sequence models by allowing the decoder to dynamically focus on relevant encoder outputs. Both Bahdanau and Luong attention improve BLEU scores compared to the vanilla encoderâ€“decoder model.

---

## ğŸ‘©â€ğŸ’» Author

Machine Learning Lab â€“ Experiment 6  
Sequence-to-Sequence Learning with Attention


